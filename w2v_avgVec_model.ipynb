{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Movie Sentiment Analasis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use the IMDB movie reviews dataset, provided on Kaggle (https://www.kaggle.com/c/word2vec-nlp-tutorial/data), to build a sentiment analysis model using NLP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having established a baseline score for our model using the BoW implementation, we will now imply transfer learning by using the Word2Vec algorithm proposed by Google to conduct sentiment analysis. In this approach, we will use the average feature vectors of each word to build our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd # to read the csv datasets and import them into python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data\" # path to the data folder in your directory\n",
    "# import the labeled and unlabeled training data to train our model\n",
    "label_train = pd.read_csv(data_path + \"/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3) \n",
    "unlabel_train = pd.read_csv(data_path + \"/unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3) \n",
    "# import the test data to evaluate our model\n",
    "test_data = pd.read_csv(data_path + \"/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"9999_0\"</td>\n",
       "      <td>\"Watching Time Chasers, it obvious that it was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"45057_0\"</td>\n",
       "      <td>\"I saw this film about 20 years ago and rememb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"15561_0\"</td>\n",
       "      <td>\"Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7161_0\"</td>\n",
       "      <td>\"I went to see this film with a great deal of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"43971_0\"</td>\n",
       "      <td>\"Yes, I agree with everyone on this site this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>\"18984_0\"</td>\n",
       "      <td>\"The original Man Eater by Joe D'Amato is some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>\"16433_0\"</td>\n",
       "      <td>\"When Home Box Office was in it's early days m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>\"16006_0\"</td>\n",
       "      <td>\"Griffin Dunne was born into a cultural family...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>\"40155_0\"</td>\n",
       "      <td>\"Not a bad story, but the low budget rears its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>\"35270_0\"</td>\n",
       "      <td>\"This not-very-good mummy-alien flick does fea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                             review\n",
       "0       \"9999_0\"  \"Watching Time Chasers, it obvious that it was...\n",
       "1      \"45057_0\"  \"I saw this film about 20 years ago and rememb...\n",
       "2      \"15561_0\"  \"Minor Spoilers<br /><br />In New York, Joan B...\n",
       "3       \"7161_0\"  \"I went to see this film with a great deal of ...\n",
       "4      \"43971_0\"  \"Yes, I agree with everyone on this site this ...\n",
       "...          ...                                                ...\n",
       "49995  \"18984_0\"  \"The original Man Eater by Joe D'Amato is some...\n",
       "49996  \"16433_0\"  \"When Home Box Office was in it's early days m...\n",
       "49997  \"16006_0\"  \"Griffin Dunne was born into a cultural family...\n",
       "49998  \"40155_0\"  \"Not a bad story, but the low budget rears its...\n",
       "49999  \"35270_0\"  \"This not-very-good mummy-alien flick does fea...\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabel_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup # to get rid of the HTML tags in the reviews\n",
    "import re # to remove punctuations and numericals from the review\n",
    "\n",
    "from nltk.corpus import stopwords # to remove the stop words in our reviews and obtain our tokenizer\n",
    "import nltk.data\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "nltk.download()\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle') # to convert reviews into list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_review(unclean_review, remove_stopwords=False):\n",
    "    \"\"\" \n",
    "    Function that takes a single unclean review from the original dataset\n",
    "    and returns a cleaned and preprocessed version of it. \n",
    "    Input: string: an uncleaned review from the dataset\n",
    "    Output: string: cleaned and preprocessed review\n",
    "    \"\"\"\n",
    "    # removes the HTML tags in the review\n",
    "    untagged_review = BeautifulSoup(unclean_review).get_text() \n",
    "    # removes everything not in A-Z or a-z and replaces it with a space\n",
    "    letter_only_review = re.sub(\"[^a-zA-Z]\", \" \", untagged_review) \n",
    "    # converting everything to lowercase\n",
    "    letter_only_review = letter_only_review.lower() \n",
    "    # converting everything to tokenized words\n",
    "    words_review = letter_only_review.split() \n",
    "    if remove_stopwords:\n",
    "        # converting to set for faster access\n",
    "        stop_words = set(stopwords.words(\"english\")) \n",
    "        # removing all the stop words in the review\n",
    "        words_review = [w for w in words_review if not w in stop_words] \n",
    "    return words_review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    \"\"\"\n",
    "    Function that takes in a review and returns it in the form of a list of its sentences.\n",
    "    Input: string: a review from the dataset\n",
    "    Output: list of list: list of sentences where each sentence list is a list of words\n",
    "    \"\"\"\n",
    "    # splitting review into sentences\n",
    "    sentences = tokenizer.tokenize(review.strip())\n",
    "    list_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if len(sentence) > 0:\n",
    "            # clean up the sentence by preprocessing it\n",
    "            list_sentences.append(preprocess_review(sentence, remove_stopwords))\n",
    "    return list_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yasoo\\Anaconda3\\envs\\walmart\\lib\\site-packages\\bs4\\__init__.py:311: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yasoo\\Anaconda3\\envs\\walmart\\lib\\site-packages\\bs4\\__init__.py:311: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yasoo\\Anaconda3\\envs\\walmart\\lib\\site-packages\\bs4\\__init__.py:385: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yasoo\\Anaconda3\\envs\\walmart\\lib\\site-packages\\bs4\\__init__.py:385: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yasoo\\Anaconda3\\envs\\walmart\\lib\\site-packages\\bs4\\__init__.py:385: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yasoo\\Anaconda3\\envs\\walmart\\lib\\site-packages\\bs4\\__init__.py:311: UserWarning: \"b'... ...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yasoo\\Anaconda3\\envs\\walmart\\lib\\site-packages\\bs4\\__init__.py:311: UserWarning: \"b'....'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yasoo\\Anaconda3\\envs\\walmart\\lib\\site-packages\\bs4\\__init__.py:385: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yasoo\\Anaconda3\\envs\\walmart\\lib\\site-packages\\bs4\\__init__.py:311: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yasoo\\Anaconda3\\envs\\walmart\\lib\\site-packages\\bs4\\__init__.py:385: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yasoo\\Anaconda3\\envs\\walmart\\lib\\site-packages\\bs4\\__init__.py:311: UserWarning: \"b'.. .'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yasoo\\Anaconda3\\envs\\walmart\\lib\\site-packages\\bs4\\__init__.py:385: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "for review in label_train[\"review\"]: \n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n",
    "for review in unlabel_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will be training our model using the Word2Vec algorithm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the hyperparamaters as per the information provided by Google's doc at https://code.google.com/archive/p/word2vec/\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "threads = 6       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# training the model\n",
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec(sentences, workers=threads, size=num_features, \n",
    "                          min_count=min_word_count, window=context, sample=downsampling)\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# saving the model\n",
    "model.save(\"default_w2v\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "# model trained on the 75,000 reviews provided in the Kaggle Database\n",
    "model = Word2Vec.load(\"models\\default_w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averageVec(words, model, num_features):\n",
    "    \"\"\"\n",
    "    The function will take all the word vectors in a review and then average them \n",
    "    \"\"\"\n",
    "    featureVec = np.zeros((num_features, ), dtype=\"float32\")\n",
    "    num_words = 0\n",
    "    index_set = set(model.wv.index2word) # converted to set for faster access\n",
    "    # if a word in the review is in the model's vocab then add its featureVec to the total\n",
    "    for word in words:\n",
    "        if word in index_set:\n",
    "            num_words += 1\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "    # get the average vector\n",
    "    featureVec = np.divide(featureVec, num_words)\n",
    "    return featureVec\n",
    "\n",
    "def batchAvgVecs(reviews, model, num_features):\n",
    "    \"\"\" \n",
    "    Given a list of reviews, it calculates the average feature vector for review and returns\n",
    "    a list of these feature vectors\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    result = np.zeros((len(reviews), num_features), dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        result[i] = averageVec(review, model, num_features)\n",
    "        if i % 5000 == 0:\n",
    "            print(\"{} reviews processed\".format(i))\n",
    "        i += 1\n",
    "    return result\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-94-66d74581dd4e>:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  featureVec = np.add(featureVec, model[word])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 reviews processed\n",
      "5000 reviews processed\n",
      "10000 reviews processed\n",
      "15000 reviews processed\n",
      "20000 reviews processed\n",
      "0 reviews processed\n",
      "5000 reviews processed\n",
      "10000 reviews processed\n",
      "15000 reviews processed\n",
      "20000 reviews processed\n"
     ]
    }
   ],
   "source": [
    "# cleaning up the datasets and calculate their avg feature vectors \n",
    "\n",
    "clean_train_data = []\n",
    "for review in label_train[\"review\"]:\n",
    "    clean_train_data.append(preprocess_review(review, remove_stopwords=True))\n",
    "trainAvgVecs = batchAvgVecs(clean_train_data, model, num_features)\n",
    "\n",
    "clean_test_data = []\n",
    "for review in test_data[\"review\"]:\n",
    "    clean_test_data.append(preprocess_review(review, remove_stopwords=True))\n",
    "testAvgVecs = batchAvgVecs(clean_test_data, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train a random forest on the average feature vectors\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "forest = forest.fit(trainAvgVecs, label_train[\"sentiment\"])\n",
    "\n",
    "# test the model and output the results\n",
    "result = forest.predict(testAvgVecs)\n",
    "output = pd.DataFrame(data={\"id\":test_data[\"id\"], \"sentiment\":result} )\n",
    "output.to_csv( \"W2V_AvgVec.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "walmart",
   "language": "python",
   "name": "walmart"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
